{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************\n",
    "## ASSIGNMENT 2\n",
    "### REINFORCEMENT LEARNING\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************\n",
    "### Abstract\n",
    "I have used FrozenLake environment to demonstrate the Reinforcement Learning. We would be using different hyper-parameters to get the average number of steps and rewards. \n",
    "*********************************\n",
    "### Import The Libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import statistics\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose environment from gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows = size of state space\n",
    "# Number of columns = size of action space\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 4 actions for FrozenLake-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "image/png": {
       "height": 200,
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='arrow-9-512.png',width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize q-table with all zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space_size,action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************\n",
    "## BASELINE MODEL\n",
    "*********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize baseline model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000 \n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.7 # alpha\n",
    "discount_rate = 0.8 # gamma\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False # Just keep tracks of whether or not the episode has finished\n",
    "    rewards_current_episode = 0 # Since we start with 0 as rewards for any new episode\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        #Exploration - Exploitation Tradeoff\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold>exploration_rate):\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state,reward,done,info = env.step(action)\n",
    "\n",
    "        #print(new_state, done, reward)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "        # Exploration rate decay\n",
    "\n",
    "    #episode +=1\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.20200000000000015\n",
      "2000 :  0.2880000000000002\n",
      "3000 :  0.3730000000000003\n",
      "4000 :  0.3770000000000003\n",
      "5000 :  0.33900000000000025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how the agent performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Steps In This Episode: 5\n",
      "Exploration Rate 0.01\n",
      "Average Number Of Steps In All Episodes: 21\n"
     ]
    }
   ],
   "source": [
    " # from each state according to the Q-table\n",
    "steps_in_each_episode = []\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "         \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    print(\"Number Of Steps In This Episode:\",step)\n",
    "    print(\"Exploration Rate\",exploration_rate)\n",
    "    \n",
    "    steps_in_each_episode.append(step)\n",
    "    \n",
    "env.close()\n",
    "print(\"Average Number Of Steps In All Episodes:\",statistics.mean(steps_in_each_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "### Let's check for a new policy function, idea is to check how the agent behaves, I am going to use argmin instead of argmax, I believe the agent will fall into Hole as soon as possible\n",
    "**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000 \n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.7 # alpha\n",
    "discount_rate = 0.8 # gamma\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Steps In This Episode: 3\n",
      "Average Number Of Steps In All Episodes: 3.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "#env.render()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False # Just keep tracks of whether or not the episode has finished\n",
    "    rewards_current_episode = 0 # Since we start with 0 as rewards for any new episode\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        #Exploration - Exploitation Tradeoff\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold>exploration_rate):\n",
    "            action = np.argmin(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state,reward,done,info = env.step(action)\n",
    "\n",
    "        #print(new_state, done, reward)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "        # Exploration rate decay\n",
    "\n",
    "    #episode +=1\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "\n",
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "steps_in_each_episode = []\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmin(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "         \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    print(\"Number Of Steps In This Episode:\",step)\n",
    "    \n",
    "    steps_in_each_episode.append(step)\n",
    "    \n",
    "env.close()\n",
    "print(\"Average Number Of Steps In All Episodes:\",statistics.mean(steps_in_each_episode))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "I tried updating the Q-table with custom rewards, whenever the done == False, it means the environment is running and the agent \n",
    "has not fallen in a hole. I tried giving it negative rewards, but the total rewards are going way to low in this case. Also, I checked\n",
    "for state == 15 which means that the agent has reached the Goal and gave it maximum reward, other than this state if the environment is\n",
    "complete and the state is something else than 15 then it means that the agent has fallen in a hole. I tried giving much negative reward\n",
    "here. The number of steps to reach the goal decreases in this case but the rewards are quite low.\n",
    "#####         Update Q-table for Q(s,a)\n",
    "#####        if done == False:\n",
    "#####            reward = -0.05\n",
    "#####        else:\n",
    "#####            if new_state == 15:\n",
    "#####                reward = 10.0\n",
    "#####            else:\n",
    "#####                reward = -1.0\n",
    "********************************************\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the variables\n",
    "\n",
    "num_episodes = 10000 \n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1 # alpha\n",
    "discount_rate = 0.99 # gamma\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "rewards_all_episodes = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False # Just keep tracks of whether or not the episode has finished\n",
    "    rewards_current_episode = 0 # Since we start with 0 as rewards for any new episode\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        #Exploration - Exploitation Tradeoff\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if(exploration_rate_threshold>exploration_rate):\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        \n",
    "        if done == False:\n",
    "            reward = -0.05\n",
    "        else:\n",
    "            if new_state == 15:\n",
    "                reward = 10.0\n",
    "            else:\n",
    "                reward = -1.0\n",
    "\n",
    "        #print(new_state, done, reward)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "        # Exploration rate decay\n",
    "\n",
    "    #episode +=1\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "                                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  -0.9632499999999955\n",
      "2000 :  0.21079999999999935\n",
      "3000 :  2.269600000000002\n",
      "4000 :  3.3885999999999994\n",
      "5000 :  4.100700000000008\n",
      "6000 :  4.302850000000001\n",
      "7000 :  4.224400000000008\n",
      "8000 :  4.431850000000003\n",
      "9000 :  4.261250000000006\n",
      "10000 :  4.284450000000007\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n",
      "Number Of Steps In This Episode: 10\n",
      "Average Number Of Steps In All Episodes: 16\n"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "steps_in_each_episode = []\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "         \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    print(\"Number Of Steps In This Episode:\",step)\n",
    "    \n",
    "    steps_in_each_episode.append(step)\n",
    "    \n",
    "env.close()\n",
    "print(\"Average Number Of Steps In All Episodes:\",statistics.mean(steps_in_each_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## Establish a baseline performance. How well did your RL Q-learning do on your problem?\n",
    "Given the baseline model, the agent is falling into the hole for most of the time. While we change some hyperparameters, the performance increases by more than 50%.\n",
    "*********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## What are the states, the actions and the size of the Q-table?\n",
    "States is the number of tiles in the Frozen Lake environment.\n",
    "Actions is the actions that can be take, i.e. Up, Down, Left Right.\n",
    "Size of Q-Table: 16X4\n",
    "*********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## What are the rewards? Why did you choose them?\n",
    "For Stepping on Empty tile, reward = -0.05\n",
    "For Stepping on Hole = -1\n",
    "For Stepping on Goal = 10\n",
    "\n",
    "I had to choose the Goal reward significantly high otherwise the Total Rewards will lead to only Negatives, as the \"G\" is farthest from \"S\". \n",
    "*********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## How did you choose alpha and gamma in the following equation? \n",
    "I tried various values of alpha and gamma\n",
    "*********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## Try a policy other than maxQ(s', a'). How did it change the baseline performance? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried using np.argmin(s',a'). As thought it completely disturbs the baseline performance by making agent move directly to Hole.\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************\n",
    "## How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?\n",
    "\n",
    "I started with the baseline information given for decay rate and epsilon. I chose a lot lesser value for decay rate which is 0.001. The value of epsilon or exploration rate when reached max steps was 0.01.\n",
    "*********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "## What is the average number of steps taken per episode?\n",
    "Average steps taken: 16\n",
    "Also, the agent never fell into Hole.\n",
    "**********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Does Q-learning use value-based or policy-based iteration?\n",
    "Q-learning uses value-based iteration. It is also called off-policy algorithm.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## What is meant by expected lifetime value in the Bellman equation?\n",
    "Instead of choosing a state based on immediate reward, agent chooses the next step in such a way that the expected reward at the end of episode is maximum. It is tuned by gamma.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Conclusion\n",
    "We can see that by evaluating the environment and customizing the rewards we get, we are getting far better results than the baseline model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## References\n",
    "Bellman Equation Basics for Reinforcement Learning: https://www.youtube.com/watch?v=14BfO5lMiuk\n",
    "\n",
    "simoninithomas Deep_reinforcement_learning_Course: https://github.com/simoninithomas/Deep_reinforcement_learning_Course\n",
    "\n",
    "Reinforcement Learning - Introducing Goal Oriented: https://www.youtube.com/watch?v=nyjbcRQ-uQ8&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
    "\n",
    "Bellman Equation: https://en.wikipedia.org/wiki/Bellman_equation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Copyright 2020 Avinash Chourasiya\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
